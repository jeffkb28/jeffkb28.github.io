---
title: "Not Like Us, Hunty: Measuring Perceptions and Behavioral Effects of Minoritized Anthropomorphic Cues in LLMs"
authors: #Jeffrey Basoah, Dr. Katharina Reinecke, Dr. Daniela Rosner, Dr. Ihudiya Finda Ogbonnaya-Ogburu
collection: publications
category: conferences
permalink: /publication/NotLikeUs
excerpt: "This study explores how Black American participants envision futures with AI through design fiction workshops, revealing narratives of “hopeful failure” where AI’s limitations open new social possibilities. The findings highlight five key areas of AI engagement, offering insights into the broader social impacts of AI development."
date: "2025-04-28"
show_date: false
venue: #"Under Review" #ACM CHI Conference on Human Factors in Computing Systems (CHI '25)"
slidesurl: #'http://academicpages.github.io/files/slides3.pdf'
paperurl: "http://jeffreybasoah.github.io/files/NotLikeUs.pdf"
citation: #"Basoah, J., Reinecke, K., Rosner, D., & Ogbonnaya-Ogburu, I. Hopeful Failure: How Collaborative Design Fiction Reimagines AI. Under review for the ACM CHI Conference on Human Factors in Computing Systems (CHI '25)."
---

As large language models (LLMs) increasingly adapt and personalize to diverse sets of users, there is an increased risk of systems appropriating \textit{sociolects}, i.e., language styles or dialects that are associated with specific minoritized lived experiences (e.g., African American English, Queer slang). In this work, we examine whether sociolect usage by an LLM agent affects user reliance on its outputs and user perception (satisfaction, frustration, trust, and social presence). We designed and conducted user studies where 498 African American English (AAE) speakers and 487 Queer slang speakers performed a set of question-answering tasks with LLM-based suggestions in either standard American English (SAE) or their self-identified sociolect. Our findings showed that sociolect usage by LLMs influenced both reliance and perceptions, though in some surprising ways. Results suggest that both AAE and Queer slang speakers relied more on the SAE agent, and had more positive perceptions of the SAE agent. Yet, only Queer slang speakers felt more social presence from the Queer slang agent over the SAE one, whereas only AAE speakers preferred and trusted the SAE agent over the AAE one. These findings emphasize the need to test for behavioral outcomes rather than simply assuming that personalization would leave to better and safer reliance outcome. They also highlight the nuanced dynamics of minoritized language in machine interactions, underscoring the need for LLMs to be carefully designed to respect cultural and linguistic boundaries while fostering genuine user engagement and trust.

<p><strong>Authors: <span style="color: #7851A9; font-weight: bold;"> Jeffrey Basoah </span>, </strong> <a href="https://chechelnitskd.github.io" target="_blank">Daniel Chechelnitsky</a>, <a href="https://iamtaolong.github.io/" target="_blank">Tao Long</a>, <a href="https://cs.stanford.edu/~katezhou/" target="_blank">Kaitlyn Zhou</a>, <a href="https://markjdiaz.com/" target="_blank">Dr. Mark Díaz</a>, <a href="https://maartensap.com/" target="_blank">Dr. Maarten Sap</a></p>